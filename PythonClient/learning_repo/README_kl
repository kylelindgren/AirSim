# ===================================================================================
# 	training results
# ===================================================================================
Notes from Andrew Ng:
Deciding What to Do Next
	Getting more training examples: Fixes high variance
	Trying smaller sets of features: Fixes high variance
	Adding features: Fixes high bias
	Adding polynomial features: Fixes high bias
	Decreasing λ: Fixes high bias
	Increasing λ: Fixes high variance

a3c implementation: https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/

test_20_net_123
	epochs: 200
		final train values: variance
			loss: 0.0729 - acc: 0.9023 - val_loss: 0.1758 - val_acc: 0.8004
		fails every time - immediately veers right but then left into pillar 2

test_20_net_1243
	epochs: 100
		final train values: variance
			loss: 0.0933 - acc: 0.8782 - val_loss: 0.1560 - val_acc: 0.8058
		fails every time - traverses between pillars 1-2, veers right narrowly catching 3
	epochs: 200
		final train values: high variance
			loss: 0.0395 - acc: 0.5177 - val_loss: 0.2237 - val_acc: 0.7268
		successfully snakes through the pillars for 20/20 episodes
		successfully veers left past pillars for 17/20 episodes when y+-1.5 start
			failed (3) for all y+1.5 start
--- after self.set_z = 0.97 (previously 0.0) ---
--- after self.forward_vel = 0.15 (previously 0.20) ---
	imit_20_net_1243_100
		final train values:
			loss: 0.0874 - acc: 0.8896 - val_loss: 0.2595 - val_acc: 0.7013
		y-1.5: hits pillar 1
		y+0: hits right wall
		y+1.5: success
	imit_20_net_1243_200
		final train values:
			loss: 0.0706 - acc: 0.9170 - val_loss: 0.2042 - val_acc: 0.7723
		hits pillar 1 when y-1.5 start, other start locs takes right path



# ===================================================================================
# 	design of experiment
# ===================================================================================

random init home position
	take different paths through pillars

# ===================================================================================
# ===================================================================================

anaconda env info: https://www.reddit.com/r/Python/comments/45aczf/anaconda_not_able_to_import_the_packages_like/

main.py
	read config_main.ini
	set agent (human, imitation, intervention, dqn - nn)
	run experiment mode (training nn models, testing, training human)

support.py
experiment mode: train_human()
	loop for given # of episodes
		resets reward, time, observation
		loop for given # of steps
			select action based on current observation
			record past observation
			save past observation and action taken
			execute selected action, get new observation and reward
			work on replay when recording human data
			save reward and action taken
			check if goal or if reached any other simulation limit


# ===================================================================================
# 	sample operation
# ===================================================================================
human behavior
	config_main.ini
		n_episodes = 10
		run_id = meow
		agent = human
		ground = True
		cv_mode = True
		exp = 0
	python main.py
		human avoids obstacles for n_episodes
		results saved in ../data folder
			run_id_imit_n.csv
			run_id_avg_n.csv
train network
	python neural.py
		run_id = run_id from config_main.ini
		var_id = variation of run_id
		n_episodes = val from config_main.init
		output stored in ../neural_models folder
			neural_name provided in save_neural() inside train_model()
test network
	config_main.ini
		agent = imitation
		all other parameters same
	learn.py
		class ImitationAgen()
			__init__()
				self.model = load_neural(name='neural_name', ...)
				self.lstm = True/False if model contains lstm modules
	python main.py


# ===================================================================================
# 	ros turtlebot operation
# ===================================================================================
human behavior
	config_main.ini
		n_episodes = 10
		run_id = meow
		agent = human
		ros = True
		ground = True
		cv_mode = True
		exp = 0
	.bashrc
		export ROS_MASTER_URI=http://charlie:11311/  # charlie = 192.168.88.106
		export ROS_IP=192.168.88.57
	roslaunch turtlebot_teleop keyboard_teleop_charlie.launch 
	charlie
		connect to ASUS network
			pwd labZ2C37
		ssh ubuntu@charlie (charlie defined as 192.168.88.106 in /etc/hosts)
			pwd autonomous
		.bashrc
			export ROS_MASTER_URI=http://kyle-vaio:11311/  # kyle-vaio = 192.168.88.57
			export ROS_IP=192.168.88.106
		~/catkin_ws/src/zed-ros-wrapper/src/zed_wrapper_nodelet.cpp
			line ~247 defines openni_depth_mode values 2
		~/catkin_ws/src/zed-ros-wrapper/launch/zed_camera.launch
			...
			<param name="resolution"            value="3" />
		    <param name="quality"               value="1" />
		    <param name="sensing_mode"          value="0" />
		    <param name="frame_rate"            value="30" />
		    <param name="odometry_db"           value="" />
		    <param name="openni_depth_mode"     value="2" />
		    <param name="gpu_id"                value="-1" />
			...
		source ~/catkin_ws/devel/setup.bash
		roslaunch turtlebot_bringup cam_minimal.launch
	python main.py
		command from keyboard_teleop_charlie.launch terminal
		human avoids obstacles for n_episodes
		results saved in ../data folder
			run_id_imit_n.csv
			run_id_avg_n.csv
train network
	python neural.py
		run_id = run_id from config_main.ini
		var_id = variation of run_id
		n_episodes = val from config_main.ini
		output stored in ../neural_models folder
			neural_name provided in save_neural() inside train_model()
test network
	config_main.ini
		agent = imitation
		all other parameters same
	learn.py
		class ImitationAgen()
			__init__()
				self.model = load_neural(name='neural_name', ...)
				self.lstm = True/False if model contains lstm modules
	python main.py
